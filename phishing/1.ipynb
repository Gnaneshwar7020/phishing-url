{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio of 25 components: 0.8543\n",
      "Epoch 1/2, Loss: 0.0422, Training Accuracy: 98.93%\n",
      "Epoch 2/2, Loss: 0.0213, Training Accuracy: 99.55%\n",
      "\n",
      "Test Accuracy: 99.51%\n",
      "Predictions saved to model_predictions.csv\n",
      "Model saved to tabular_feature_extractor1.pth\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [1, 256]           6,656\n",
      "            Conv1d-2               [1, 32, 256]             192\n",
      "       BatchNorm1d-3               [1, 32, 256]              64\n",
      "              GELU-4               [1, 32, 256]               0\n",
      "            Conv1d-5               [1, 32, 256]           1,024\n",
      "       BatchNorm1d-6               [1, 32, 256]              64\n",
      "              GELU-7               [1, 32, 256]               0\n",
      "            Conv1d-8               [1, 32, 256]             160\n",
      "       BatchNorm1d-9               [1, 32, 256]              64\n",
      "             GELU-10               [1, 32, 256]               0\n",
      "           Conv1d-11               [1, 32, 256]           1,024\n",
      "      BatchNorm1d-12               [1, 32, 256]              64\n",
      "             GELU-13               [1, 32, 256]               0\n",
      "CustomResidualUnit-14               [1, 32, 256]               0\n",
      "           Conv1d-15               [1, 32, 256]           1,024\n",
      "      BatchNorm1d-16               [1, 32, 256]              64\n",
      "             GELU-17               [1, 32, 256]               0\n",
      "           Conv1d-18               [1, 32, 256]             160\n",
      "      BatchNorm1d-19               [1, 32, 256]              64\n",
      "             GELU-20               [1, 32, 256]               0\n",
      "           Conv1d-21               [1, 64, 256]           2,048\n",
      "      BatchNorm1d-22               [1, 64, 256]             128\n",
      "             GELU-23               [1, 64, 256]               0\n",
      "CustomResidualUnit-24               [1, 64, 256]               0\n",
      "           Conv1d-25                [1, 1, 256]             193\n",
      "      BatchNorm1d-26                [1, 1, 256]               2\n",
      "             GELU-27                [1, 1, 256]               0\n",
      "        MaxPool1d-28                 [1, 1, 85]               0\n",
      "           Linear-29                    [1, 48]           4,128\n",
      "          Dropout-30                    [1, 48]               0\n",
      "           Linear-31                     [1, 2]              98\n",
      "================================================================\n",
      "Total params: 17,221\n",
      "Trainable params: 17,221\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.70\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 1.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchsummary import summary\n",
    "\n",
    "# CustomResidualUnit (unchanged)\n",
    "class CustomResidualUnit(nn.Module):\n",
    "    def __init__(self, input_filters, output_filters):\n",
    "        super(CustomResidualUnit, self).__init__()\n",
    "        self.pointwise1 = nn.Conv1d(input_filters, input_filters, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.norm1 = nn.BatchNorm1d(input_filters)\n",
    "        self.depthwise_conv = nn.Conv1d(input_filters, input_filters, kernel_size=5, stride=1, padding=2, groups=input_filters, bias=False)\n",
    "        self.norm2 = nn.BatchNorm1d(input_filters)\n",
    "        self.pointwise2 = nn.Conv1d(input_filters, output_filters, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.norm3 = nn.BatchNorm1d(output_filters)\n",
    "        self.act = nn.GELU()\n",
    "        self.use_shortcut = input_filters == output_filters\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        out = self.pointwise1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.depthwise_conv(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.pointwise2(out)\n",
    "        out = self.norm3(out)\n",
    "        if self.use_shortcut:\n",
    "            out += shortcut\n",
    "        return self.act(out)\n",
    "\n",
    "# TabularFeatureExtractor (updated to handle 25 features)\n",
    "class TabularFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_features=25, num_outputs=2, dropout_prob=0.3):\n",
    "        super(TabularFeatureExtractor, self).__init__()\n",
    "        self.dense1 = nn.Linear(num_features, 256)\n",
    "        self.conv_initial = nn.Conv1d(1, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.norm_initial = nn.BatchNorm1d(32, track_running_stats=False)\n",
    "        self.act_initial = nn.GELU()\n",
    "        self.res_unit1 = CustomResidualUnit(32, 32)\n",
    "        self.res_unit2 = CustomResidualUnit(32, 64)\n",
    "        self.conv_final = nn.Conv1d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm_final = nn.BatchNorm1d(1)\n",
    "        self.act_final = nn.GELU()\n",
    "        self.pooling = nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "        self.dropout_layer = nn.Dropout(dropout_prob)\n",
    "        self.dense2 = nn.Linear(85, 48)  # Note: this might need adjustment based on output shape\n",
    "        self.dense3 = nn.Linear(48, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x).unsqueeze(1)\n",
    "        x = self.conv_initial(x)\n",
    "        x = self.norm_initial(x)\n",
    "        x = self.act_initial(x)\n",
    "        x = self.res_unit1(x)\n",
    "        x = self.res_unit2(x)\n",
    "        x = self.conv_final(x)\n",
    "        x = self.norm_final(x)\n",
    "        x = self.act_final(x)\n",
    "        x = self.pooling(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.nn.functional.gelu(self.dense2(x))\n",
    "        x = self.dropout_layer(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Load and preprocess data\n",
    "file_path = \"/Users/gnaneshwarkandula/Downloads/phishing/50000_samples_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract features, labels, and URLs\n",
    "feature_cols = [f'f{i}' for i in range(1, 56)]  # f1 to f55\n",
    "features = df[feature_cols].values\n",
    "labels = df['label'].values\n",
    "urls = df['URL'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, urls_train, urls_test = train_test_split(\n",
    "    features, labels, urls, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Standardize features before PCA\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA to reduce to 25 features\n",
    "pca = PCA(n_components=25)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Explained variance ratio of 25 components: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "# Create train and test datasets\n",
    "train_dataset = TabularDataset(X_train_pca, y_train)\n",
    "test_dataset = TabularDataset(X_test_pca, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TabularFeatureExtractor(num_features=25, num_outputs=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5 \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_predictions = []\n",
    "test_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        test_total += targets.size(0)\n",
    "        test_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "        test_true_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Save results to a CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'URL': urls_test,\n",
    "    'True_Label': test_true_labels,\n",
    "    'Predicted_Label': test_predictions\n",
    "})\n",
    "results_df.to_csv('model_predictions.csv', index=False)\n",
    "print(\"Predictions saved to model_predictions.csv\")\n",
    "\n",
    "# Save the model\n",
    "model_path = \"tabular_feature_extractor1.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Corrected Model Summary\n",
    "summary(model, input_size=(25,), batch_size=1)  # Adjusted to match raw input before unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pca.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "joblib.dump(pca, \"pca.pkl\")\n",
    "# Load later: scaler = joblib.load(\"scaler.pkl\"), pca = joblib.load(\"pca.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
